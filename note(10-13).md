## Fast neural subgraph matching

In this section, we are going to solve the following task: Given a query graph $Q$ and a target graph $T$, determine whether $Q$ is isomorphic to a subgraph of $T$. 

**Overview of NeuroMatch**

NeuroMatch adopts a two-stage process: *embedding stage* where $G_T$ is decomposed into many small overlapping graphs and each graph is embedded. And *query stage* where query graph is compared to the target graph directly in the embedding space. 

**Embedding stage**. For every node $u$ in $G_T$, we extract the $k$-hop neighborhood $G_u$. GNN then maps node $u$ into an embedding $z_u$. By using a $k$-layer GNN to embed node $u$, we can essentially capture the $k$-hop neighborhood structure around the center node $u$. 

**Query stage**. We design a *subgraph prediction function* $f(z_q,z_u)$ that predicts whether $G_Q$ anchored at $q$ (or the $k$-hop neighborhood of $q$ ) is a subgraph of $G_u$. We assume that $k$ is at least the diameter of the query graph, to allow the information of all nodes to be propagated to the anchor node( $k=10$ in many real-world graph). 

**Subgraph prediction function**

We enforce the embedding space to directly capture the subgraph relations. In particular, NeuroMatch satisfies the following properties for subgraph relations: 

- *Transitivity*: If $G_1$ is a subgraph of $G_2$ and $G_2$ is a subgraph of $G_3$, then $G_1$ is a subgraph of $G_3$
- *Anti-symmetry*: If $G_1$ is subgraph of $G_2$, $G_2$ is a subgraph of $G_1$ iff they are isomorphic. 
- *Intersection set*: The intersection of the subgraph of $G_1$ and the subgraph of $G_2$ contains all common subgraphs of $G_1$ and $G_2$. 

Order embedding ensures that the subgraph relations are properly reflected in the embedding space: if $G_q$ is a subgraph of $G_u$, then the embedding $z_q$ of node $q$ has to be the **lower-left** of $z_u$: 
$$
z_q[i]\leq z_u[i] \ \forall_{i=1}^{D}  \ \ \ \text{iff} \ \ \ G_q\subseteq G_u 
$$
where $D$ is the embedding dimension. We thus train GNN by minimizing the max margin loss: 
$$
L = \sum_{(z_u,z_q)\in P} E(z_q, z_u) + \sum_{(z_u,z_q\in N)} \max(0, \alpha  - E(z_q,z_u))\\
E(z_q,z_u) = \big||\max\{0,z_q - z_u\}|\big|^2_2
$$
where $P$ denotes the positive examples and $N$ denotes the negative examples. A violation of subgraph constraint happens when $z_q[i]>z_u[i]$, and $E(z_q,z_u)$ represents its magnitude.

For positive examples, $E(z_q,z_u)$ is minimized when all elements in query embeddings is less than that in target embeddings. For negative pairs, the violation $E(z_q,z_u)$ must go beyond *margin* $\alpha$, in order to have zero loss. 

We can easily observe that if the $k$-hop neighborhood of $u$ is a subgraph of $k$-hop neighborhood of $v$, then $\forall s\in N_u, \exist t\in N_v$ such that $(k-1)$-hop neighborhood of $s$ must be the $(k-1)$-hop neighborhood of $t$. To maintain this property, we should use sum-based neighborhood aggregation in GNN. 

**Matching Nodes via Voting**

Since the embedding space is only an approximation of subgraph constraints, $G_q$ may not be subgraph of $G_u$ even if $z_q\leq z_u$. Our insight is that matching a pair of anchor nodes imposes constraints on the neighborhood structure of the pair. 

>Observation: Let $N^{(l)}$ denotes the $l$-hop neighborhood. Then, if $q\in G_Q$ and node $u\in G_T$ match, $\forall i\in N^{(k)}_q,\exist j\in N^{(l)}_u,l\leq k$ such that node $i$ and node $j$ match.  

Because all paths in $G_Q$ have corresponding paths in $G_T$, the shortest distance of a node $i$ to $q$ is at most the shortest distance of $j$ to $q$. Therefore, we can check whether there exists a node $j$ that matches $i$ within the distance. 

<img src = "C:\Users\16549\AppData\Roaming\Typora\typora-user-images\image-20240322142400432.png" width = 500>

**Training NeuroMatch**

**Training data.** To achieve high generalization performance on unseen queries, we train the network with the randomly generated queries. For a positive pair, we sample $G_u\in G_T$ and $G_q\in G_u$. To sample $G_u$, we first select a node $u\in G_T$ and perform a random BFS. 

The sampler traverse each edge in BFS with a fixed probability. After this, we perform the same random BFS on $G_u$ to obtain $G_q$. 

Given a positive pair $(G_q,G_u)$, we use the following approach to generate negative examples. The first type of negative samples are created by randomly choose $q,u\in G_T$ and perform BFS. The second type is generated by perturbing the $G_q$ to make it no longer a subgraph of $G_u$. 

**Curriculum.** We introduce a curriculum training scheme that improves performance. We first train the model on a small number of easy queries and then train on successively more complex queries with increased batch size.

## Position-wise GNN

**Anchor-set**

In position-aware task, nodes are labeled by their positions in the graph. For example: 

<img src = "C:\Users\16549\AppData\Roaming\Typora\typora-user-images\image-20240325195429295.png" width = 450>

Traditional GNN will assign $v_1$ and $v_2$ same embedding, due to structure symmetry. Here we propose *Position-wise Graph Neural Networks* for computing node embedding that incorporate a node's positional information with respect to all other nodes in the network. 

<span style="color:purple">Goal:</span> Embed the metric space $(V,d)$ into the Euclidean space $\mathbb R^{k}$ such that the original distance metric is preserved. That is to say, for every node pair $u,v\in V$, the Euclidean embedding distance $\big ||z_u-z_v|\big|_2$ is close to the origin distance $d(u,v)$. 

Consider the following embedding function of node $v\in V$ : 
$$
f(v) = (\frac{d(v,S_{1,1})}{k},\frac{d(v,S_{1,2})}{k}...,\frac{d(v,S_{\log n,c\log n})}{k})
$$
where: 

- $c$ is a constant and $k = c\log^2n$ represents the number of *anchor-set* $S_{i,j}$
- $S_{i,j}\subset V$ is chosen by including each node in $V$ independently with probability $\frac{1}{2^i}$
- $d(v,S_{i,j}) = \min_{u\in S_{i,j}} d(v,u)$

It can be proved that the embedding distance produced by $f$ is provably close to the original distance metric $(V,d)$. 

**Message Computation function $F$**

Message computation function $F(v,u,h_v,h_u)$ has to account for both position-based similarities as well as feature information. The message flows from node $u$ in *anchor-set* to arbitrary $v\in V$. 

Position-based similarity is the key to reveal a node's positional information. We propose the following $q$-hop shortest path to compute position-based similarity: 
$$
d^q(u,v) = \begin{cases}d(v,u) & d(v,u)\leq q\\ \infin &\text{otherwise}\end{cases}
$$
Since we aim to map nodes that are close in the network to similar embeddings, we further transform the distance $s(v,u) = \frac{1}{1+d^q(v,u)}$. 

For function $F$, simple product works well empirically: 
$$
F(v,u,h_v,h_u) = s(v,u)\text{concat}(h_v,h_u)
$$
**Framework**

<img src = "C:\Users\16549\AppData\Roaming\Typora\typora-user-images\image-20240326113744407.png" width = 500>

Algorithm $1$ summarizes the general framework of P-GNNs. Concretely, a P-GNN layer first samples $k$ random anchor-set $S_i$. The $i$ dimension of node embedding $z_v$ represents messages computed with respect to the anchor-set $S_i$. 

The massage from each anchor-set is computed independently. Firstly, the massages are gathered into a temporal matrix $\mathcal{M}_i\in\mathbb{R}^{|S_i|\times 2r}$. Next, **mean** aggregation function with trainable parameters is applied to obtain $\mathbf{M}_v[i]\in\mathbb R^{r}$ (aggregation and projection). 

Finally, we apply a non-linear transformation to get $z_v\in \mathbb R^{k}$ via trainable weights $\mathbf{w} \in \mathbb{R}^r$ and non-linearity $\sigma$. The output embeddings $z_v$ are position-aware, as each dimension of the embedding encodes the necessary information to distinguish structurally equivalent nodes that reside in different parts of the graph.

But we can't feed the output node embedding $z_v$ from the previous layer to the next layer because the dimension of $z_v$ is arbitrary permuted. Therefore, P-GNNs also compute structure-aware massage $h_v\in \mathbb R^{r}$, which is computed via order-invariant aggregation. 

## GNN for recommenders 

**Loss functions**

Assume that we have a set of positive edges $E$ (observed user-item interactions) and negative edges $E_{neg}=\{(u,v)|(u,v)\not \in E\}$. Consider the following binary loss function: 
$$
L = -\frac{1}{|E|}\sum_{(u,v)\in E} \log \sigma \big(f_\theta(u,v)\big) - \frac{1}{|E_{neg}|}\sum_{(u,v)\in E_{neg}} \log \Big(1-\sigma\big(f_\theta(u,v)\big)\Big)
$$
In the binary loss, the scores of **all** positive edges are pushed higher than **all** negative edges. However, it may not align well with recall metric since it penalizes model predictions even if the training recall metric is perfect. 

The binary loss considers across all users at one time while the recall metric is defined for each user. Therefore, the surrogate loss function should be defined in a personalized manner. Bayesian Personalized Ranking (BPR) loss achieves this. 

Concretely, for each user $u$, we want the score of rooted positive edges $E(u)$ to be higher than those of rooted negative edges $E_{neg}(u)$: 
$$
L = \frac{1}{|E(u)|\cdot |E_{neg}(u)|}\sum_{(u,v_{pos})\in E(u)}\sum_{(u,v_{neg})\in E_{neg}(u)} -\log\sigma\big(f_\theta(u,v_{pos}) - f_{\theta}(u,v_{pos})\big)
$$
In training process, we sample a mini-batch $U_{mini}$. For each node $u\in U_{mini}$, we sample **one** positive item $v_{pos}$ and a set of negative example $V_{neg}$:
$$
L = \frac{1}{|U_{mini}|}\sum_{u\in U_{mini}}\frac{1}{|V_{neg}|}\sum_{v_{neg}\in V_{neg}} -\log\sigma\big(f_\theta(u,v_{pos}) - f_{\theta}(u,v_{pos})\big)
$$
**Neural Graph Collaborative Filtering**

Conventional collaborative filtering model is based on shallow encoders, where high-order graph structure is not explicitly captured. 

We want a model that explicitly captures graph structure. GNNs are a natural approach to achieve this. Specifically, we can use a GNN to generate graph-aware user/item embeddings. 

**LightGCN**

LightGCN is an approach to make GCN faster while maintains its expressivity. 

<span style="color:purple">Observation:</span> Shallow learnable embeddings are expressive enough for performance. This is because shallow embeddings $O(ND)$ $\gg$ GNN parameters $O(D^2)$. 

Therefore, we can simplify the GNN used in NGCF by removing its parameters. Let $D$ be the degree matrix of $A$. Define the normalized adjacency matrix $\widetilde A$ as: 
$$
\widetilde A = D^{-\frac{1}{2}}AD^{-\frac{1}{2}}
$$
Each layer of GCN's aggregation can be written in a matrix form: 
$$
E^{(k+1)} = \text{ReLU}(\widetilde AE^{(k)}W^{(k)})
$$
Simplify GCN by removing ReLU non-linearity: 
$$
\begin{align}
E^{(k+1)} &= \widetilde AE^{(k)}W^{(k)}\\
&= \widetilde A^{k}E(W^{(0)}W^{(1)}...W^{(k)})\\
&= \widetilde A^{k}EW
\end{align}
$$
LightGCN applies $E\leftarrow \widetilde A^{k}E$ for $K$ times. Each matrix multiplication diffuses the current embeddings to their one-hop neighbors. Next, we consider multi-scale diffusion: 
$$
\alpha_0E^{(0)}+\alpha_1E^{(1)}+\alpha_2E^{(2)}....+\alpha^{K}E^{(K)}
$$
where $a_k = \frac{1}{K+1}$ for all $k=0,1...K$. 

Similar users share many common neighbors and are expected to have similar future preferences. The diffusion directly encourages the  embeddings of similar users/items to be  similar.